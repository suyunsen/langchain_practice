"""
 -*- coding: utf-8 -*-
time: 2023/10/26 16:15
author: suyunsen
email: suyunsen2023@gmail.com
"""

import json
import os
import time
import sys

import gradio as gr

from options import parser
from typing import Optional
import torch
import numpy as np
import random
import asyncio
from langchain.callbacks import AsyncIteratorCallbackHandler

sys.path.append("../../")

from QA.govermentQA import GoQa
from Custom.ChatGLM import ChatGlm26b
from Custom.Custom_SparkLLM import Spark
from Custom.BaiChuan import Baichuan

history = []
readable_history = []
cmd_opts = parser.parse_args()

templet_prompt = """
         å‡å¦‚ä½ æ˜¯ä¸€åé—®ç­”ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ä¸€ä¸‹ç»™å‡ºçš„å†…å®¹æ‰¾å‡ºé—®é¢˜çš„æ­£ç¡®ç­”æ¡ˆã€‚
         ç­”æ¡ˆåªå­˜åœ¨ç»™å‡ºçš„å†…å®¹ä¸­ï¼Œä½ çŸ¥é“å°±å›ç­”ï¼Œä¸è¦è‡ªå·±ç¼–é€ ç­”æ¡ˆã€‚
         å› ä¸ºä½ æ˜¯é—®ç­”ä¸“å®¶ä½ éœ€è¦ä»”ç»†åˆ†æé—®é¢˜å’Œç»™å‡ºçš„å†…å®¹ï¼Œä¸è¦ç»™å‡ºé”™è¯¯ç­”æ¡ˆï¼Œä¸è¦ç»™å‡ºå¤šä½™çš„ç­”æ¡ˆã€‚
         è®°ä½ä½ åªéœ€è¦æŒ‰ç»™å‡ºçš„å†…å®¹ä½œç­”ï¼Œä¸éœ€è¦è‡ªå·±æ€»ç»“ã€‚
         è®°ä½å¦‚æœä½ è®¤ä¸ºé—®é¢˜å’Œç»™å‡ºçš„å†…å®¹ç›¸å…³æ€§ä¸å¤§ï¼Œä½ éœ€è¦å›å¤ï¼šè¯·è¯¦ç»†è¯´æ˜æ‚¨å’¨è¯¢é—®é¢˜çš„åœ°å€å’ŒåŠç†çš„äº‹é¡¹ã€‚
         ä»¥ä¸‹æ˜¯ç»™å‡ºçš„å†…å®¹ï¼š{context}
         é—®é¢˜æ˜¯:{question}
        """
llm = Baichuan()
sparkllm:Optional[Spark] = None
Baichuanllm:Optional[Baichuan] = None
chatglmllm:Optional[ChatGlm26b] = None
qa_chain = GoQa(llm=llm,templet_prompt=templet_prompt)

Baichuanllm = llm

chat_h = """<h2><center>ChatGLM WebUI</center></h2>"""
spark_h = """<h2><center>Spark WebUI</h2>"""
Baichuan_h = """<h2><center>Baichuan WebUI</h2>"""
Baichuan_h_2 = """<h2><center>Baichuan WebUI 2</h2>"""
head_h =Baichuan_h

_css = """
#del-btn {
    max-width: 2.5em;
    min-width: 2.5em !important;
    height: 2.5em;
    margin: 1.5em 0;
}
"""


def prepare_model():
    global model
    if cmd_opts.cpu:
        model = model.float()
    else:
        if cmd_opts.precision == "fp16":
            model = model.half().cuda()
        elif cmd_opts.precision == "int4":
            model = model.half().quantize(4).cuda()
        elif cmd_opts.precision == "int8":
            model = model.half().quantize(8).cuda()

    model = model.eval()


# prepare_model()


def parse_codeblock(output):
    lines = output['response'].split("\n")
    for i, line in enumerate(lines):
        if "```" in line:
            if line != "```":
                lines[i] = f'<pre><code class="{lines[i][3:]}">'
            else:
                lines[i] = '</code></pre>'
        else:
            if i > 0:
                lines[i] = "<br/>" + line.replace("<", "&lt;").replace(">", "&gt;")
    source = "\n\n"
    source += "".join(
        [f"""<details> <summary>å‡ºå¤„ [{i + 1}]</summary>\n"""
         f"""{doc.page_content}\n"""
         f"""</details>"""
         for i, doc in
         enumerate(output['source'])])
    return "".join(lines)+source

def predict(query, max_length, top_p, temperature):
    llm.set_llm_temperature(temperature)
    output = ''
    if head_h == Baichuan_h:
        output = qa_chain.get_no_konwledge_answer(query)
    else :
        output = qa_chain.ask_question(query,int(top_p))
    readable_history.append((query, parse_codeblock(output)))
    return  readable_history


def save_history():
    if not os.path.exists("./outputs"):
        # os.mkdir("./outputs")
        pass
    s = [{"q": i[0], "o": i[1]} for i in history]
    filename = f"save-{int(time.time())}.json"
    with open(os.path.join("outputs", filename), "w", encoding="utf-8") as f:
        f.write(json.dumps(s, ensure_ascii=False))


def load_history(file):
    global history, readable_history
    try:
        with open(file.name, "r", encoding='utf-8') as f:
            j = json.load(f)
            _hist = [(i["q"], i["o"]) for i in j]
            _readable_hist = [(i["q"], parse_codeblock(i["o"])) for i in j]
    except Exception as e:
        print(e)
        return readable_history
    history = _hist.copy()
    readable_history = _readable_hist.copy()
    return readable_history


def clear_history():
    if head_h == Baichuan_h:
        Baichuanllm.history = []
    history.clear()
    readable_history.clear()
    return gr.update(value=[])

def load_chatGlm():
    global chatglmllm
    if  chatglmllm is None:
        chatglmllm = ChatGlm26b()
    qa_chain.set_llm_modle(chatglmllm)
    head_h = chat_h
    La = gr.HTML(head_h)
    return La,clear_history()


def load_spark():
    global sparkllm
    if sparkllm is None:
        sparkllm = Spark()
    qa_chain.set_llm_modle(sparkllm)
    head_h = spark_h
    La = gr.HTML(head_h)
    return La,clear_history()

def load_baichuan():
    global Baichuanllm
    if Baichuanllm is None:
        Baichuanllm = Baichuan()
    Baichuanllm.set_baichuanmodel(1)
    qa_chain.set_llm_modle(Baichuanllm)
    head_h = Baichuan_h
    La = gr.HTML(head_h)
    return La,clear_history()

def load_baichuan2():
    global Baichuanllm
    if Baichuanllm is None:
        Baichuanllm = Baichuan()
    Baichuanllm.set_baichuanmodel(2)
    qa_chain.set_llm_modle(Baichuanllm)
    head_h = Baichuan_h_2
    La = gr.HTML(head_h)
    return La,clear_history()


def create_ui():
    with gr.Blocks(css=_css) as demo:
        prompt = "è¾“å…¥ä½ çš„å†…å®¹..."
        with gr.Row():
            with gr.Column(scale=3):
                La = gr.HTML(head_h)
                with gr.Row():
                    with gr.Column(variant="panel"):
                        with gr.Row():
                            max_length = gr.Slider(minimum=4, maximum=4096, step=4, label='Max Length', value=2048)
                            top_p = gr.Slider(minimum=1, maximum=15, step=1, label='æ£€ç´¢è¿”å›Top P', value=5)
                        with gr.Row():
                            temperature = gr.Slider(minimum=0.01, maximum=1.0, step=0.01, label='Temperature', value=0.01)

                        # with gr.Row():
                        #     max_rounds = gr.Slider(minimum=1, maximum=50, step=1, label="æœ€å¤§å¯¹è¯è½®æ•°ï¼ˆè°ƒå°å¯ä»¥æ˜¾è‘—æ”¹å–„çˆ†æ˜¾å­˜ï¼Œä½†æ˜¯ä¼šä¸¢å¤±ä¸Šä¸‹æ–‡ï¼‰", value=20)

                with gr.Row():
                    with gr.Column(variant="panel"):
                        with gr.Row():
                            clear = gr.Button("æ¸…ç©ºå¯¹è¯ï¼ˆä¸Šä¸‹æ–‡ï¼‰")

                        with gr.Row():
                            save_his_btn = gr.Button("ä¿å­˜å¯¹è¯")
                            load_his_btn = gr.UploadButton("è¯»å–å¯¹è¯", file_types=['file'], file_count='single')

                        with gr.Row():
                            chatGLM_load = gr.Button("åŠ è½½chatGLMæ¨¡å‹")
                            spark_load = gr.Button("åŠ è½½æ˜Ÿç«æ¨¡å‹")

                        with gr.Row():
                            baichuan_load = gr.Button("åŠ è½½ç™¾å·æ¨¡å‹")
                            baichuan_load_2 = gr.Button("åŠ è½½ç™¾å·æ¨¡å‹2")
            with gr.Column(scale=7):
                chatbot = gr.Chatbot(elem_id="chat-box", show_label=False).style(height=500)
                with gr.Row():
                    message = gr.Textbox(placeholder=prompt, show_label=False, lines=2)
                    clear_input = gr.Button("ğŸ—‘ï¸", elem_id="del-btn")

                with gr.Row():
                    submit = gr.Button("å‘é€")

        submit.click(predict, inputs=[
            message,
            max_length,
            top_p,
            temperature
        ], outputs=[
            chatbot
        ])

        clear.click(clear_history, outputs=[chatbot])
        clear_input.click(lambda x: "", inputs=[message], outputs=[message])

        chatGLM_load.click(load_chatGlm,outputs=[La,chatbot])
        spark_load.click(load_spark,outputs=[La,chatbot])

        baichuan_load.click(load_baichuan,outputs=[La,chatbot])
        baichuan_load_2.click(load_baichuan2,outputs=[La,chatbot])

        save_his_btn.click(save_history)

        load_his_btn.upload(load_history, inputs=[
            load_his_btn,
        ], outputs=[
            chatbot
        ])

    return demo


ui = create_ui()
ui.queue().launch(
    server_name="0.0.0.0",
    server_port=cmd_opts.port,
    share=cmd_opts.share
)

